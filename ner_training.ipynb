{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa3fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertConfig\n",
    "\n",
    "from torch import nn\n",
    "from transformers import BertModel, BertPreTrainedModel\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef20be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-SUBSCRIPTION\": 1,\n",
    "    \"I-SUBSCRIPTION\": 2,\n",
    "    \"B-DATE\": 3,\n",
    "    \"I-DATE\": 4,\n",
    "    \"B-PRICE\": 5,\n",
    "    \"I-PRICE\": 6\n",
    "}\n",
    "id2label = {v: k for k, v in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245713dc",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "with open(\"nerdataset00.txt\", \"r\") as f:\n",
    "    dataset_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d19fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7d86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, label2id):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        tokens = item[\"tokens\"]\n",
    "        labels = item[\"labels\"]\n",
    "\n",
    "        encoding = self.tokenizer(tokens,\n",
    "                                  is_split_into_words=True,\n",
    "                                  return_offsets_mapping=True,\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=128)\n",
    "\n",
    "        word_ids = encoding.word_ids()\n",
    "        label_ids = []\n",
    "\n",
    "        prev_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != prev_word_idx:\n",
    "                if word_idx < len(labels): \n",
    "                    label_ids.append(self.label2id[labels[word_idx]])\n",
    "                else:\n",
    "                    label_ids.append(-100)  # fallback\n",
    "            else:\n",
    "                if word_idx < len(labels):\n",
    "                    label = labels[word_idx]\n",
    "                    if label.startswith(\"B-\"):\n",
    "                        label = label.replace(\"B-\", \"I-\")\n",
    "                    label_ids.append(self.label2id[label])\n",
    "                else:\n",
    "                    label_ids.append(-100)\n",
    "            prev_word_idx = word_idx\n",
    "        encoding.pop(\"offset_mapping\", None)\n",
    "        encoding[\"labels\"] = label_ids\n",
    " \n",
    "        return {key: torch.tensor(val) for key, val in encoding.items()}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21bdd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = NERDataset(dataset_json, tokenizer, label2id)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1830f3",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class StrongNERModel(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier = nn.Linear(config.hidden_size, num_labels)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "            # reshape to (batch_size * seq_len, num_labels)\n",
    "            loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if labels is not None else logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860b3b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_labels=len(label2id))\n",
    "model = StrongNERModel.from_pretrained(\"bert-base-uncased\", config=model_config, num_labels=len(label2id))\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.0000001)\n",
    "total_steps = len(dataloader) * 5  \n",
    "#scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.1 * total_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(45):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    for batch in loop:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss, logits = model(input_ids, attention_mask, labels=labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} average loss: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6b82ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'nermodel1.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
